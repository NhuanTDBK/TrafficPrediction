{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import sys\n",
    "import time\n",
    "from os import listdir\n",
    "\n",
    "class TrafficExtract():\n",
    "    def __init__(self):\n",
    "        self.conn = sqlite3.connect('trafficdb')\n",
    "        self.c = self.conn.cursor()\n",
    "        self.table_name = \"timetraffic\"\n",
    "        self.table_workload = \"workload\"\n",
    "        self.c.execute(\"DELETE FROM %s\"%self.table_workload)\n",
    "    def record_traffic(self,raw_data_name):\n",
    "        print \"Processing %s\"%raw_data_name\n",
    "        bulk_size = 50000\n",
    "        start_time = time.time()\n",
    "        raw_data_chunk = pd.read_csv(raw_data_name,names=[\"Timestamp\"],chunksize=bulk_size)\n",
    "        for raw_data in raw_data_chunk:\n",
    "            self.record_traffic_by_data(raw_data)\n",
    "        end_time = time.time()\n",
    "        print end_time - start_time\n",
    "    def record_traffic_by_data(self,raw_data):\n",
    "        \n",
    "        # In[ ]:\n",
    "        # dataCount = np.array(np.zeros(length))\n",
    "        print \"Flush out all data...\"\n",
    "        self.c.execute(\"DELETE FROM %s\"%self.table_name)\n",
    "        length = raw_data.shape[0]\n",
    "        tmp_data = raw_data\n",
    "        try:\n",
    "            mapFunction = lambda x: (int(x),1)\n",
    "            list_count =[(item[0][0],item[0][1]) for item in tmp_data.applymap(mapFunction).values.tolist()]\n",
    "            self.conn.executemany(\"INSERT INTO %s(timestamp,count) VALUES (?,?)\"%self.table_name,list_count)\n",
    "        except Exception as ex:\n",
    "            for i in np.arange(0,length):\n",
    "                try:\n",
    "                    index = raw_data.irow(i)[\"Timestamp\"]\n",
    "                    self.c.execute('INSERT INTO %s VALUES (%d,%d)'%(self.table_name,int(index),1))\n",
    "                except Exception as e:\n",
    "                    print index\n",
    "                    pass\n",
    "        dt = self.c.execute('select time,count(time) from %s group by time'%self.table_name).fetchall()\n",
    "        for item in dt:\n",
    "            self.c.execute('INSERT INTO %s VALUES (%d,%d)'%(self.table_workload,int(item[0]),int(item[1])))\n",
    "        self.conn.commit()\n",
    "    def readFolder(self,folder_name):\n",
    "        files = listdir(folder_name)\n",
    "        total = len(files)\n",
    "        for (index, filename) in enumerate(files):\n",
    "            print \"Reading %s\"%filename\n",
    "            print \"Processing %d/%d\"%(index+1,total)\n",
    "            self.record_traffic(\"%s/%s\"%(folder_name,filename))\n",
    "        self.finalize()\n",
    "    def finalize(self):\n",
    "        self.conn.commit()\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker = TrafficExtract()\n",
    "worker.readFolder(sys.argv[0])\n",
    "# # start = timeit.default_timer()\n",
    "# # worker.record_traffic(\"data_part_9310\")\n",
    "# # stop = timeit.default_timer()\n",
    "# # print stop - start \n",
    "# worker.record_traffic(\"ita/output/data.csv\")\n",
    "# worker.conn.execute(\"DELETE FROM %s\"%worker.table_workload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tmp_data = raw_data\n",
    "# bulk_size = 50001\n",
    "# start_idx = 0\n",
    "# length = raw_data.shape[0]\n",
    "# bulk_patch = length/bulk_size+1\n",
    "# mapFunction = lambda x: (int(x),1)\n",
    "# for i in np.arange(bulk_patch):\n",
    "#     print \"Bulking %d/%d\"%(i,bulk_patch)\n",
    "#     end_idx = start_idx+bulk_size\n",
    "#     if(end_idx>=length):\n",
    "#         end_idx = length\n",
    "#     tmp_data = raw_data[start_idx:end_idx]\n",
    "#     list_count =[(item[0][0],item[0][1]) for item in tmp_data.applymap(mapFunction).values.tolist()]\n",
    "#     worker.conn.executemany(\"INSERT INTO %s(timestamp,count) VALUES (?,?)\"%worker.table_name,list_count)\n",
    "#     start_idx += bulk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"/home/nhuan/Tools/ita_public_tools/datavalidation.csv\",names=[\"Timestamp\"],chunksize=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n",
      "Flush out all data...\n"
     ]
    }
   ],
   "source": [
    "for chunk in raw_data:\n",
    "    worker.record_traffic_by_data(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}